- [CPU是如何设计出来的](#cpu是如何设计出来的)
  - [CPU的内部结构](#cpu的内部结构)
- [计算机体系结构](#计算机体系结构)
- [CPU性能提升：Cache机制](#cpu性能提升cache机制)
  - [Cache的工作原理](#cache的工作原理)
  - [一级`Cache`和二级`Cache`](#一级cache和二级cache)
  - [不是所有的处理器都有Cache](#不是所有的处理器都有cache)
- [CPU性能提升：流水线](#cpu性能提升流水线)
  - [流水线工作原理](#流水线工作原理)
  - [超流水线技术](#超流水线技术)
  - [流水线冒险](#流水线冒险)
    - [结构冒险](#结构冒险)
    - [数据冒险](#数据冒险)
    - [控制冒险](#控制冒险)
  - [分支预测](#分支预测)
  - [乱序执行](#乱序执行)
  - [SIMD](#simd)
  - [单发射和多发射](#单发射和多发射)
- [多核CPU](#多核cpu)
  - [单核处理器的瓶颈](#单核处理器的瓶颈)
  - [big.LITTLE结构](#biglittle结构)
  - [超线程技术](#超线程技术)
  - [CPU核数的选择](#cpu核数的选择)
- [异构计算](#异构计算)
  - [什么是异构计算](#什么是异构计算)
  - [GPU](#gpu)
  - [DSP](#dsp)
  - [FPGA](#fpga)
  - [TPU](#tpu)
  - [NPU](#npu)
- [总线和地址](#总线和地址)
  - [地址的本质](#地址的本质)
  - [总线的概念](#总线的概念)
  - [总线编址方式](#总线编址方式)
- [指令集与微架构](#指令集与微架构)
  - [什么是指令集](#什么是指令集)
  - [什么是微架构](#什么是微架构)
  - [指令助记符：汇编语言](#指令助记符汇编语言)

# CPU是如何设计出来的

不同架构的`CPU`都仅能支持有限个指令，任何的复杂的运算都可以分解成有限个基本指令来完成。

## CPU的内部结构

|CPU内部主要构成|工作原理|
|-|-|
|算数逻辑单元`ALU`|由算数单元和逻辑单元组成。算数单元主要负责数学运算，逻辑单元主要负责逻辑运算。|
|控制单元|控制单元根据程序计数器（`PC`）中的地址，会不断地从内存`RAM`中取指令，放到指令寄存器中并进行译码，将指令中的操作码和操作数分别送到`ALU`，执行相应的运算。|
|寄存器|为了效率考虑，运算结果一般会先保存到寄存器中，然后由控制单元将该数据从寄存器存储到内存`RAM`中。至此，一条指令流程就结束了。|
|`Cache`|由于`CPU`频率远高于内存频率，引入缓存。<li>当`CPU`到`RAM`中读取数据时，一次缓存一批数据到`Cache`中，下次`CPU`取指令和数据时，就可以先检查缓存中是否有数据，如果没有数据，则再到内存中取读取；<li>当`CPU`写数据到`RAM`中，可以先将数据暂时存储到`Cache`中，等待时机再将数据刷新到内存中；<li>`Cache`大大提高了`CPU`的访问效率|

`CPU`的跳转指令实现：根据`ALU`的运算结果和输出的`Flags`标志位，直接修改`PC`寄存器的地址即可。跳转指令的实现，可以实现更加复杂的程序逻辑，如程序的分支、循环等结构。

目前的`CPU`设计，一般都是使用`VHDL`或`Verilog`硬件描述语言来整合`ALU`、内存控制单元、寄存器、`Cache`等电路模块，然后通过电子设计自动化`EDA`工具进行仿真、修改和验证，将其转化为逻辑门电路。只需要关注数字电路的逻辑功能实现，而具体的物理电路实现，布线和连接则由`EDA`工具自动完成。

 
# 计算机体系结构

计算机系统一般采用内存+外存的存储结构：程序指令保存在如磁盘、`NAND Flash`、`SD卡`等外部存储器中，当程序运行时，相应的程序会首先加载到内存，然后`CPU`从内存一条一条取指令、翻译指令和运行指令。


依据保存方式的不同，计算机架构可以分为以下两类：
|计算机架构分类|说明|
|-|-|
|冯.诺依曼架构|<li>指令和数据混合存储，存储在同一块存储器上的不同物理地址上；<li>结构简单，工程上容易实现|
|哈弗架构|<li>指令和数据分开独立存储，分别存放在程序存储器和数据存储器；<li>指令和数据可以在一个时钟周期内并行访问，运行效率更高；<li>`CPU`实现复杂。|
|混合架构|由于`CPU`中引入了`Cache`，包括`指令Cache`和`数据Cache`，结合了上面两种架构的优点，称为混合架构。|

# CPU性能提升：Cache机制

`CPU`的工作频率高于内存的工作频率，虽然可以提高内存`RAM`的频率，但是和一些高端的`CPU`相比，还是存在差距。于是就引入了`Cache缓存机制`。有速度瓶颈的地方就有缓存。

## Cache的工作原理

`Cache`在物理实现上就是静态随机访问存储器（`SARM`），运行速度介于`CPU`和内存`DRAM`之间。工作原理就是通过自有的存储空间，缓存一部分内存中的指令和数据，减少`CPU`访问内存的次数，从而提高系统的整体性能。

1. 读数据：
> 缓存命中：`CPU`读取内存中地址的数据时，会现在缓存`Cache`中寻找，如果找到称为缓存命中。如果不在缓存中，则缓存未命中，`CPU`就会向内存中读取数据。并将内存中从该地址开始的一片数据到`Cache`中。

2. 写数据：
当`CPU`往地址A的内存写入数据时，并未真正写入`RAM`中，而是暂时写入了`Cache`中，缓存的空间一般会有一个特殊的标记位`Dirty Bit`，记录变化。当缓存需要刷新时，如`Cache`满，`CPU`需要缓存新数据时，会检查`Dirty Bit`的变化，并将这些变化的数据写回到`RAM`中，再去缓存新的内存数据。

## 一级`Cache`和二级`Cache`

如果缓存命中，就无需访问内存，效率大大提升；如果缓存未命中，`CPU`就需要到内存中重新读取数据，而且还要缓存一片新的数据到`Cache`中，如果`Cache`满了，还需要清理`Cache`，如果`Cache`中的数据有`Dirty Bit`，还要写回到内存中。严重影响了`CPU`的读写效率。

通过在一级`Cache`和内存之间增加二级`Cache`，缓解这种问题。


现在的`CPU`一般都是集成多个核（`Core`），每个`Core`都会有自己独立的`L1 Cache`，包括`D-Cache`和`I-Cache`。

在`x86`架构的`CPU`，一般每个`Core`也会有自己独立的`L2 Cache`，`L3 Cache`被所有的`Core`共享。而在`ARM`架构的`CPU`中，`L2 Cache`则被每簇的`Core`共享。


## 不是所有的处理器都有Cache

一些单片机芯片等`ARM`处理器都没有`Cache`。原因有三个：
- 这些处理器本身低功耗，增加`Cache`会增加功耗，而且会增加芯片的面积和成本；
- 这些处理器本身的工作频率就不高，和`RAM`之间不存在带宽问题；
- `Cache`无法保证实施性，缓存未命中时，`CPU`从`RAM`中读取数据的时间是不确定的，在嵌入式的实时控制场景中无法接受。


# CPU性能提升：流水线

## 流水线工作原理

> 一条指令的执行，一般要经过取指令、翻译指令、执行指令3个基本流程。

`CPU`内部的电路分为不同的单元：取址单元、译码单元、执行单元等。

假设每个步骤的执行时间都是一个时钟周期，那么一条指令执行完成需要3个时钟周期。

在`CPU`执行指令的3个时钟周期内，取址单元只在第一个时钟周期里面工作，其余两个时钟周期空闲。其他的两个执行单元也是如此。效率很低。引入流水线。

1. 第一个时钟周期取址单元工作，译码单元和执行单元可以空闲；
2. 第二个时钟周期，当译码单元在翻译指令1时，取址单元去取指令2，执行单元空闲；
3. 第三个时钟周期，当执行单元在执行指令1时，译码单元与翻译指令2，取址单元去取指令3；
4. 以此这样类推。

引入流水线之后，虽然一条指令的执行流程和时间不变，是3个时钟周期，但是从流水线输出来看，差不多平均每个时钟周期就可以执行一条指令。提高了`CPU`的性能。

> 流水线的本质是拿空间换时间，让不同指令的各个小步操作重叠，通过多条指令的并行执行，加快程序整体运行效率。


## 超流水线技术

流水线存在木桶效应，只需要找出耗时最长的那道工序，对其进行细分，拆解为更多的工序即可以。

流水线中的每一级电路单元一般都是由组合逻辑电路和寄存器组成，组合逻辑电路用来执行本道工序的逻辑运算，寄存器用来保存运算输出结果，并作为下一道工序的输入。

流水线通过减少每一道工序的时间来提升整条流水线的效率。通过缩短一个时钟周期来提升效率，即减少每条指令所耗费的时间。一个时钟周期的时间变短，`CPU`的主频也就相应提升（主频是时间周期的倒数）。

> 超流水线结构：5级以上的流水线。

提升`CPU`主频的本质在于减少流水线中每一级流水的执行时间。解决办法有3个：
1. 优化流水线中各级流水线的性能（最难）；
2. 依靠半导体制造工艺；
3. 不断增加流水线深度，流水线越深，流水线各级时延就越小，更容易提升主频；


流水线变深也会带来一些问题：
1. 流水线越深，电路就会越复杂，芯片面积就会越大，功耗随之上升；
2. 当程序中存在跳转、分支结构时，下面预取的指令就要全部丢掉，需要到跳转的地方重新取指令执行；
3. 流水线越深，一旦预取指令失败，浪费和损失就会越严重，流水线发生停顿，无法按照预期执行，称为流水线冒险。

## 流水线冒险

|流水线冒险分类||
|-|-|
|结构冒险|所需的硬件正在为前面的指令工作|
|数据冒险|当前指令需要前面指令的运算数据才能够执行|
|控制冒险|需要根据之前指令的执行结果决定下一步的行为|


### 结构冒险

如果多条指令都用相同的硬件资源，如内存单元、寄存器等，就会发生冲突。如下面的汇编程序

    ADD R2,R1,R0
    SUB R1,R4,R3
两条指令执行时都需要访问寄存器`R1`，在硬件资源上发生了冲突，称为结构冒险。解决方法就是直接对冲突的寄存器进行重命名即可。

    ADD R2,R1,R0
    SUB R5,R4,R3

### 数据冒险

当前指令的执行需要上一条指令的运算结果，上一条指令没有运行结束，当前指令就无法运行，只能暂停执行。

    ADD R2,R1,R0
    SUB R4,R2,R3

解决方法：使用`operand forwarding`技术，当第一条指令执行结束后不再执行写寄存器的操作，而是直接使用运算结果；或者在两条指令之间插入空指令，虽然延缓了执行的时钟周期，但是不会将预取的指令全部丢掉，在流水线很深时，这种方法很划算。



### 控制冒险

同数据冒险，也可以采用插入空指令的方法来解决。

## 分支预测

考虑极端的情况，一个20级深度的流水线，每个指令都需要上一条指令执行结束才去执行，最多需要插入19个空指令，相当于暂停19个时钟周期。这是`CPU`无法接受的。


为了解决这种情况，现在的`CPU`流水线在取指和译码的时候，都要对跳转指令进行分析，预测可能执行的分支和路径。防止预取错误的分支路径指令给流水线带来停顿。


|分支预测分类|说明|
|-|-|
|静态预测|程序编译时通过编译器进行分支预测<li>对于循环程序最有效<li>对于跳转分支，默认不跳转，按照顺序执行<li>在编写有跳转分支的代码时（如`if`）要把大概率执行的代码分支放在前面|
|动态预测|在程序运行时预测。采用不同的算法去提高预测的准确率。在`CPU`内部，除了`Cache`之外，数分支预测器电路版图最大。|

## 乱序执行

造成流水线冲突的根本原因在于指令之间存在相关性。除了可以插入空指令之外，还可以将指令重新排序，乱序执行，将不具有相关性的指令放到前面执行，具有相关性的指令放在后面，等到执行到该条指令时，前面的指令执行结束，就不存在数据冒险。


支持乱序执行的`CPU`处理器，其内部一般会有专门的乱序执行逻辑电路，该控制电路会对当前指令的执行序列进行分析，看能否提前执行。

## SIMD

> `SIMD`：几个执行部件同时访问内存，一次性读取一条指令的所有的操作数，这种数据操作类型叫做单指令多数据。帮助`CPU`实现数据并行访问，执行效率更高。

## 单发射和多发射

> 单发射处理器：每个时钟周期只能从存储器取一条指令，每个时钟周期也只能执行一条指令。

> 多发射处理器：每个时钟周期可以执行多条指令。

# 多核CPU

## 单核处理器的瓶颈

以上的芯片都是基于单核处理器的分析。单核处理器的性能再强劲，也是在串行执行多任务，多个任务轮流占用`CPU`运行。只要任务切换的够快，就可以让用户觉得多个程序在同时运行。

多核处理器可以真正让多个任务同时执行。

## big.LITTLE结构

多个`Core`集成到一个处理器上时，当工作任务不多时，如果所有的`Core`一起跑，实际只有一个`Core`在工作，其他的在空跑，带来功耗的上升。

`ARM`推出`big.LITTLE`大小核结构，当`CPU`工作负载很重时，启用高性能的`Core`工作，当`CPU`很闲时，切换到低功耗的`Core`工作。根据不同的应用场景分配不同的`Core`工作，在性能和功耗之间达到平衡。

高性能的`Core`放到一个`Cluster`，构成`big Cluster`，低功耗的`Core`放到一个`Cluster`，构成`LITTLE Cluster`。每个`Cluster`都有自己独立的`数据Cache`和`指令Cache`，每个`Cluster`共享`L2 Cache`。为了保证多个`Cache`之间，`Cache`和`RAM`之间的数据一致性，两个`Cluster`之间还有一个缓存一致性接口相连。

## 超线程技术

> 主要应用于`Inter`，`AMD`的`X86`多核处理器上的技术。如4核8线程，6核12线程等。目前市场上没有发现使用超线程的`ARM`处理器。

超线程技术通过增加一定的逻辑控制电路，使用特殊指令可以将一个物理处理器当两个逻辑处理器使用，每个逻辑处理器都可以分配一个线程运行。最大限度地提升`CPU`的资源利用率。

## CPU核数的选择

`CPU`的核数不是越多越好。对于一些大型游戏一般侧重单核性能，主频越高，游戏体验越好；而服务器则更倾向于多核多线程。

# 异构计算

## 什么是异构计算

> 异构计算就是在`SoC`芯片内部集成不同架构的`Core`，如`DSP`、`GPU`、`NPU`等不同架构的处理单元。

`CPU`像一个大脑，适合处理分支、跳转等复杂逻辑的程序；`GPU`头脑简单，四肢发达，擅长处理图片、视频数据；在人工智能领域，则`NPU`和`FPGA`发挥作用。在一个`SoC`芯片上发挥所长。

## GPU

> `GPU`：图像处理单元。

计算机联网需要网卡，计算机显示需要显卡，显卡是显式接口卡的简称。显卡将数字图像信号转换为模拟信号，并输出到屏幕上。


早期计算机，显卡都是集成到主板上，只充当适配器的角色，只具备图形信号转换和输出的功能。`CPU`可以处理一些简单的图像处理。对于一些大型游戏，制图，视频渲染等，计算机的图像数据计算量成倍增加，`CPU`越来越力不从心，独立的显卡开始承担图像处理和视频渲染的工作。


`GPU`是显卡电路板上的芯片，主要用来图像处理，视频渲染。而且`GPU`在浮点运算，大数据处理，密码破译，人工智能等领域都是一把好手，比`CPU`更适合做大规模并行的数据运算。


`GPU`与`CPU`的不同点在于，它没有复杂的控制单元和`Cache`，却集成了几千甚至上万个计算核心。`CPU`由于`Cache`和控制单元电路就占用了很大一部分芯片面积，从而不可能集成太多的`ALU`。`GPU`天然多线程，特别适合大数据并行处理。

`GPU`一般以独立显卡的形式插到主板上。

## DSP

> `DSP`：数字信号处理器，主要用在音频信号处理和通信领域。

`DSP`与`CPU`相比的优势：
1. `DSP`采用哈佛结构，指令和数据独立存储，并行存取，执行效率更高；
2. 对指令的优化，提高了对信号的处理效率，`DSP`有专门的硬件乘法器；
3. `DSP`是专门针对信号处理，乘法等运算做了优化的`ASIC`电路，相比于`CPU`，`GPU`等通用处理器，没有冗余的逻辑电路，功耗更小。

`DSP`的缺陷是只适合做大量重复运算。无法向`CPU`那样提供一个通用的平台。

## FPGA

> `FPGA`：现场可编程门阵列。`FPGA`芯片内部集成了大量的逻辑门电路和存储器，用户可以通过`VHDL`和`Verilog`甚至高级语言来编写代码，描述它们之间的连线，将这些连线配置文件写入芯片内部，就可以构成具有特定功能的电路。

`FPGA`直接将硬件描述语言翻译为晶体管门电路的组合，实现特定的算法和功能。剔除了`CPU`，`GPU`等通用处理器的冗余逻辑电路，电路结构更加简单直接，处理速度更快，在数据并行处理方面最具优势。

可编程逻辑器件通过配套的集成开发工具，可以随时修改代码，下载到行骗内部，重新连线生成新的功能。


`FPGA`一般和`CPU`一起使用、协同工作。如高速信号采集和处理中，`CPU`负责采集模拟信号，并通过`A/D`转化成数字信号送给`FPGA`去处理，`FPGA`对数字信号进行快速处理最后将处理的结果再发回`CPU`，供`CPU`处理。


`FPGA`比`DSP`的开发更加具有灵活性，成本也更高，上手也比较难，因此主要应用在一些军事设备、高端电子设备、高速信号采集和图像处理领域。

## TPU

> `TPU`：张量处理器，谷歌公司为了提高深层网络的运算能力而专门研发的一款`ASIC`芯片。

`TPU`相对于`CPU`和`GPU`，砍去了分支预测、`Cache`、多线程等逻辑器件，集成了6万多个矩阵乘法单元和24MB的片上内存`SRAM`作为缓存。

`TPU`的运算能力相对于`CPU`和`GPU`有很大的提升，但是功耗很大。


## NPU

> `NPU`：神经网络处理器，是面向人工智能领域，基于神经网络算法，进行硬件加速的处理器统称。

# 总线和地址

`CPU`内部的寄存器是没有地址的，可以通过寄存器名访问。而内存和外部设备控制器中的寄存器都需要有一个地址，然后`CPU`才可以通过地址去读写这些外部控制器的寄存器，控制外部设备的运行。或者根据地址去读写指定的内存单元。

## 地址的本质

当`CPU`想要访问一个存储单元时，可以通过`CPU`管脚发出一组信号，经过译码器译码，选中与这个信号对应的存储单元，然后就可以直接读写这块内存了。`CPU`管脚发出的这组信号，也就是存储单元对应的编号，就是地址。

地址的本质就是由`CPU`管脚发出的一组地址控制信号。也被称为物理地址。

在一个32位的计算机系统重，32位的地址线有4GB大小的寻址空间。(2的32次方)。

寻址空间和计算机实际的内存大小并不是一回事，因为有些内存的地址是不可访问的。


在带有`MMU`（内存管理器）的`CPU`平台下，程序运行一般使用的是虚拟地址，`MMU`会把虚拟地址转换为物理地址。

## 总线的概念

早期的计算机都是`CPU`和内存`RAM`直接相连的，现在`CPU`都是通过总线和内存`RAM`，外部设备相连的。`CPU`和北桥通过`系统总线`相连，内存`RAM`和北桥通过`内存总线`相连。`CPU`和各个设备之间通过共享总线的方式进行通信。

总线就是各种数字信号的集合，包括地址信号，数据信号，控制信号等。不同的总线通过`桥`来连接。桥一般是一个芯片组电路，用来将总线的电子信号翻译成另外一种电子信号。如北桥，用来将`CPU`从系统总线发过来的电子信号转换成内存能够识别的内存总线信号。

不同的设备遵循相同的总线协议与计算机进行通信。就可以将不同厂家的设备集成到一个计算机系统中。

## 总线编址方式

内存`RAM`和外部设备都挂到同一个总线上，计算机通过两种方式进行编址：

1. 统一编址，内存`RAM`和外部设备共享`CPU`的寻址空间，`CPU`可以像操作内存`RAM`一样去读写外部设备的寄存器和内部`RAM`，如`ARM`架构的`CPU`；
2. 独立编址，内存`RAM`和外部设备的寄存器独立编址，分别占用不同的地址空间，外部设备的寄存器有独立的64KB空间，需要专门的`IN/OUT`指令才能访问，这片独立编址的64KB的空间被称为`I/O地址空间`，如`X86`架构的`CPU`。

# 指令集与微架构

不同架构的处理器支持的指令类型是不同的。`ARM`架构的处理器只支持`ARM`指令，`X86`架构的处理器只支持`X86`指令。如果在`ARM`架构的处理器上运行`X86`指令，就无法运行，报未知的错误。

`CPU`支持的有限个指令的集合，称为指令集。

## 什么是指令集

指令集最终的实现就是微架构，就是`CPU`内部的各种译码和执行电路。

`CPU`设计者和编译器开发者遵循的是同一个指令集标准，编译器最终编译生成的指令，都是`CPU`硬件电路支持运行的指令。每一个不同架构的`CPU`一般都需要配套一个对应的编译器。

指令集的价值在于大家都遵循同一个标准去开发计算机系统的不同硬件和软件，有利于整个计算机系统生态的构建。


## 什么是微架构

> 微架构：处理器架构。指令集在`CPU`内部的具体硬件电路的实现，就称为微架构。一般也称为`CPU`内核。

一套相同指令集，可以由不同形式的电路实现，可以有不同的微架构。在设计微架构的时候需要考虑诸如，处理器是否支持分支预测，单发射还是多发射，顺序执行还是乱序执行，流水线深度，几级`Cache`，`Cache`大小等。


|主流指令集架构分类|指令集授权|微架构授权（内核授权）|盈利|特点|
|-|-|-|-|-|
|`ARM`|开放指令集授权，其他公司可以基于授权的指令集去设计自己的微架构和`Soc`芯片<li>由于能力和精力限制，只有`ARM`公司和几个技术积累较深的公司会设计微架构，如如苹果公司的`swift`微架构，高通公司的`Krait`微架构，华为和龙芯等。|`ARM`公司的微架构授权（内核授权）客户比较多<li>国外的三星，飞思卡尔，德州仪器等；<li>国内的海思、瑞微芯、全志等；<li>微架构授权客户的特点是不能对`ARM`的`CPU`内核（微架构）进行修改；<li>`ARM`公司会设计出不同的微架构，甚至会开放一些可配置选项（如`Cache`大小），以便搭建出差异化的处理器；<li>即使是同一个微架构，通过不同的配置也可以设计出不同的处理器类型，如三星的`Exynos4412处理器`，瑞芯微公司的`RK3188处理器`都采用`ARM`公司设计出的`Cortex-A9`内核。|`ARM`公司不生产`CPU`，靠`IP`授权获利|<li>采用精简指令集`RISC`；<li>`ARM`结构处理器功耗更低，性能弱于`X86`；<li>`ARM`主要用于移动终端，如手机和平板电脑等|
|`X86`|`Intel`，`AMD`，和上海的兆芯，这三个厂家根据新版本的`X86`指令集设计出各自的微架构，然后基于各自的微架构设计出不同的`CPU`<li>如`Intel`的酷睿处理器，不管是`i3`，`i5`还是`i7`，都基于相同的微架构。面向市场的不同的定位和需求，在处理器主频、核数、`Cache`大小等方面进行差异性配置，`AMD`的锐龙3，锐龙5等也是如此。|不开放内核授权|`X86`指令集不授权，不开放内核，靠`X86`专利垄断制造行业壁垒，抬高其他处理器厂商的准入门槛|<li>采用复杂指令集`CISC`；<li>性能比`ARM`架构更快，功耗更大；<li>`X86`主要用于`PC`机和服务器等|




`X86`指令集因为专利垄断和授权限制，除了三家公司，其他公司一般无法获得授权去设计微架构和生产处理器。`ARM`开放指令集授权，其他公司可以自己设计微架构和处理器。




## 指令助记符：汇编语言

指令由操作码和操作数组成。指令格式是二进制的，不好记，给这些二进制指令定义各种助记符，这种助记符就是汇编指令。一段汇编程序经过汇编器的翻译，才能变成`CPU`真正识别、译码和运行的二进制指令。