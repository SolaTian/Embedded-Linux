- [嵌入式设备死机经验排查分享](#嵌入式设备死机经验排查分享)
  - [1、资源泄漏](#1资源泄漏)
    - [1.1、资源泄漏的分类简介](#11资源泄漏的分类简介)
    - [1.2、内存泄漏](#12内存泄漏)
      - [1.2.1、内存泄漏的原因](#121内存泄漏的原因)
      - [1.2.2、内存泄漏的现象](#122内存泄漏的现象)
      - [1.2.3、内存泄漏的排查](#123内存泄漏的排查)
        - [a、addr2line 工具](#aaddr2line-工具)
        - [b、coredump](#bcoredump)
        - [c、线程名](#c线程名)
        - [d、stat\_mem、/proc 等内存监控工具](#dstat_memproc-等内存监控工具)
        - [f、增加日志](#f增加日志)
        - [e、进程监控脚本 syslog.sh](#e进程监控脚本-syslogsh)
        - [f、守护进程 `daemonApp`](#f守护进程-daemonapp)
    - [1.3、文件描述符泄漏和 `Socket`泄漏](#13文件描述符泄漏和-socket泄漏)
      - [1.3.1、描述符泄漏的原因](#131描述符泄漏的原因)
      - [1.3.2、描述符泄漏的排查](#132描述符泄漏的排查)
        - [a、lsof](#alsof)
        - [b、/proc](#bproc)
        - [c、netstat](#cnetstat)
    - [1.4、线程泄漏](#14线程泄漏)
      - [1.4.1、线程泄露的原因](#141线程泄露的原因)
      - [1.4.2、线程泄露的现象及后果](#142线程泄露的现象及后果)
      - [1.4.3、线程泄露的排查](#143线程泄露的排查)
    - [1.5、锁泄漏](#15锁泄漏)
      - [1.5.1、锁泄漏的原因](#151锁泄漏的原因)
      - [1.5.2、锁泄漏的现象及后果](#152锁泄漏的现象及后果)
      - [1.5.3、锁泄漏的排查](#153锁泄漏的排查)
    - [1.6、数据库泄漏](#16数据库泄漏)
      - [1.6.1、数据库泄露的原因](#161数据库泄露的原因)
      - [1.6.2、数据库泄露的后果](#162数据库泄露的后果)
      - [1.6.3、数据库泄露的排查](#163数据库泄露的排查)
  - [2、其他可能导致程序崩溃的原因](#2其他可能导致程序崩溃的原因)
    - [2.1、栈溢出](#21栈溢出)
      - [2.1.1、栈溢出的原因](#211栈溢出的原因)
      - [2.1.2、栈溢出的排查](#212栈溢出的排查)
    - [2.2、浮点异常(`Undefined Signal 8`)](#22浮点异常undefined-signal-8)
      - [2.2.1、浮点异常的原因](#221浮点异常的原因)

# 嵌入式设备死机经验排查分享

在嵌入式开发中，常常会遇到各种各样的崩溃、卡死现象。这类问题往往非常让人头疼，因为排查起来非常困难。但是也是有一定的思路和头绪的。

下面就主要总结一下常见的排查思路。

## 1、资源泄漏

在 C 语言中，常常听到内存泄漏的危害，其实内存泄漏是资源泄漏的一种。除了内存泄漏之外，还有其他类型的泄漏会导致系统崩溃或无响应。

### 1.1、资源泄漏的分类简介

|泄露类型|直接危害|表现|
|-|-|-|
|内存泄漏|进程内存占用持续增长|崩溃、系统变慢|
|文件描述符泄漏|进程无法打开新的文件|`EMFILE（Too many open files）`|
|`socket`泄漏|端口耗尽|无法创建新连接|
|线程泄露|创建的线程/进程未正确退出或回收|线程使用数超过系统限制，`CPU` 调度效率下降|
|锁泄漏|死锁|程序卡死|
|数据库连接泄漏|连接池耗尽|数据库拒绝连接|

下面开始依次介绍这些泄漏类型

### 1.2、内存泄漏

这是最常见的导致系统卡死的现象的原因。

C 语言不像其他更高级的语言，没有内存管理机制，需要程序员手动的管理堆内存。因此容易出现内存泄漏现象。

#### 1.2.1、内存泄漏的原因


在写代码的时候，以下现象可能会导致内存泄漏：

1. `malloc()`申请的内存没有调用`free()`释放，导致内存泄漏
    ```c
    //直接泄漏
    void func() 
    {
        int *ptr = malloc(sizeof(int)*10);  // 分配后完全没有释放
    }

    //间接泄漏
    void func() 
    {
        int **ptr = malloc(sizeof(int*)*10);
        for(int i=0; i<10; i++) {
            ptr[i] = malloc(sizeof(int)*10);  // 二级指针分配
        }
        free(ptr);  // 只释放了第一级，每个ptr[i]都泄漏了
    }

    //重复释放
    int *ptr = malloc(sizeof(int));
    free(ptr);
    free(ptr);  
    ```
2. 野指针
    ```c
    //未初始化内存访问
    int *ptr;
    *ptr = 10;

    // 内存释放后继续使用指针
    int *ptr = malloc(sizeof(int)*10);
    free(ptr);    
    *ptr = 10;    // 错误：使用已释放内存（不确定行为）

    //函数返回局部变量的地址
    int* func() {
        int num = 10;
        return &num;
    }
    int *ptr = func();
    printf("value is %d\n", *ptr);

    //多个指针指向同一块内存，其中一个释放后其他指针变成野指针
    int *ptr1 = (int *)malloc(sizeof(int));
    if (ptr1 == NULL) {
        printf("内存分配失败！\n");
        return 1;
    }

    int *ptr2 = ptr1;
    int *ptr3 = ptr1;
    *ptr1 = 10;
    free(ptr1);
    printf("释放内存后 ptr2 指向的值: %d\n", *ptr2);
    printf("释放内存后 ptr2 指向的值: %d\n", *ptr2);
    ```
3. 越界: 数组越界、`memcpy()`越界，字符串函数越界
    ```c
    //数组访问越界
    int a[16] = {0};
    a[16] = 1;

    //memcpy越界
    char src[10] = "hello";
    char dest[5];
    memcpy(dest, src, 10);  

    //sprintf越界
    char buffer[10];
    sprintf(buffer, "This is a very long string");

    //strcpy越界
    char dest[10];
    strcpy(dest, "This is a very long string");
    ```


#### 1.2.2、内存泄漏的现象

当发生内存泄漏时，可能会出现以下几种的现象：

|现象|说明|举例|
|-|-|-|
|系统运行变慢，卡顿|系统可用内存逐渐减少，内存不够，程序在运行的过程中，频繁进行内存交换、分配等操作，系统响应时间变长。||
|部分功能异常|某些依赖于大量内存的功能模块可能由于内存泄漏无法工作。比如一些图像识别，数据存储等||
|系统频繁崩溃或者重启|系统可用内存几乎耗尽时，程序可能会因为无法分配到所需的内存而崩溃重启。|<li>进程内存泄露，如果访问到了非法内存，比如内核的内存地址，此时内核就会抛出一个`undefined signal 11`段错误异常，此时系统就会立刻进行重启<li>进程内存泄露，但并未访问到非法内存，只是不停占用系统的空闲堆内存而没有及时释放，当超过内核设定的系统最小空闲内存阈值时，触发`OOM`，内核默认杀死占用内存最大的进程，设备进行重启。|
|错误日志增多|系统的错误日志中可能会出现与内存相关的错误信息，如内存分配失败、访问越界等|设备`SSH`或者串口刷新一些内存异常的打印，这些打印在系统正常工作时不会出现|


#### 1.2.3、内存泄漏的排查

内存泄漏只能从上面描述的现象中进行发现。其中系统崩溃是最容易直观发现的。会伴随以下几种现象。

1. 设备无法`ping`通。
2. `SSH`连接突然断开。
3. 设备的`web`页面无法访问，访问设备的`web`页面的本质就是设备软件的`appweb`处理前端浏览器客户端的访问`web`的请求。当设备死机时，自然无法处理该请求，即无法访问`web`。

当有这几种现象时，就很有可能是内存泄露导致的设备死机。

排查思路有以下几种

|工具|获取崩溃信息|排查方法|
|-|-|-|
|有串口线|<li>设备接串口线，需要保证设备的串口波特率和`PC`端的波特率一致，否则会串口会出现乱码<li>通过一些终端软件例如`XShell`等监测设备的串口日志。|<li>当内存泄露引发`undefined signal 11`段错误时，可以通过串口日志获取挂掉的进程`PID`，`PC`和`LR`地址，结合`addr2line`工具定位到死机的是哪一个进程，位于代码中的哪一行等信息。<li>当内存泄漏引发`out of memery(OOM)`时，此时往往不易排查内存泄露的原因，可以通过使用 Linux 的实时内存工具，如 [stat_mem](../Debugging-Tools/内存工具/Linux内存工具.md#stat_mem) 工具，监控设备运行过程中各个进程使用的内存情况，找出具体是哪个进程内存消耗的越来越多。最后再排查具体的代码。|
|无串口线，仅能在设备在线时登录`SSH`|`SSH`一般无法直接获取崩溃信息。这种情况需要修改我们的主进程或者升级包内容等。<li>设备主进程新增一个调试日志保存到本地硬盘的功能，开启该功能后，等待崩溃复现，之后再将调试日志导出(注意新增该功能时，需要包括删除硬盘中调试日志的功能，否则可能会影响硬盘的数据存储)<li>设备新增保存`coredump`文件到`Linux`文件夹的功能，后续通过 U盘 挂载的方式或者 `sz` 命令取出`coredump` 文件（注意开启`coredump`文件保存功能需要关闭软件看门狗，`coredump`文件生成期间，是无法喂狗的，导致系统复位，导致`coredump`文件不完整）<li>在设备崩溃到`SSH`断开连接之间会有一小段时间，输入`dmesg`获取内核崩溃的日志。（这种方法不太适用于一些不定时没有规律的崩溃）<li>在设备底层新增监控脚本`syslog.sh`，当设备运行时，通过`SSH`执行脚本文件，监控指定的进程，当进程崩溃时，将`dmesg`信息写入到设备的目录中，再将其取出 <li>设备在编译打包的时候新增一个守护进程`daemonApp`进程，守护进程监测指定的进程，当系统出现崩溃时，写入`dmesg`信息|<li>获取到崩溃日志之后，还是和上面有串口时的排查思路一致。<li>获取到`coredump`文件之后，使用`gdb`工具进行分析崩溃的堆栈信息。|

##### a、addr2line 工具

当程序崩溃时，获得`Undefined Signal 11`错误信息，可以获得到`PC`和`LR`地址信息，可以使用`addr2line`工具分析程序发生崩溃的行数。具体可以参考

[使用 addr2line 分析 coredump 文件](../Debugging-Tools/内存工具/GDB调试.md#使用-addr2line-分析-coredump-文件)


##### b、coredump

获取到`coredump`文件时，可以使用 `GDB` 分析`coredump`文件获取程序崩溃时候的堆栈信息。具体可以参考

[使用 gdb 分析 coredump 文件](../Debugging-Tools/内存工具/GDB调试.md#使用-gdb-分析-coredump-文件)


##### c、线程名

可以在程序的线程当中增加系统调用`prctl(PR_SET_NAME)`，将线程名注册到内核中，当线程因为信号(`Undefined Signal`)崩溃时，`Linux`内核的崩溃处理逻辑会主动读取线程名称，并与其他崩溃信息(如`PID`，`TID`，`PC/LR`地址等)一起输出。如下：

```c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <pthread.h>
#include <sys/prctl.h>    // prctl()

void* thread_func(void *arg) {
    char thread_name[16] = "my_custom_thread";  // 线程名称

    // 使用 prctl 设置线程名（用于内核识别）
    if (prctl(PR_SET_NAME, (unsigned long)thread_name) == -1) {
        perror("prctl(PR_SET_NAME) failed");
        pthread_exit(NULL);
    }

    printf("Thread started with name: %s (TID: %lu)\n", thread_name, (unsigned long)pthread_self());

    // 模拟线程工作
    for (int i = 0; i < 5; i++) {
        printf("Thread %s is working... (%d/5)\n", thread_name, i + 1);
        sleep(1);
    }

    printf("Thread %s finished.\n", thread_name);
    pthread_exit(NULL);
}

int main() {
    pthread_t tid;

    // 创建线程
    if (pthread_create(&tid, NULL, thread_func, NULL) != 0) {
        perror("pthread_create failed");
        return -1;
    }

    printf("Main thread created thread (ID: %lu)\n", (unsigned long)tid);

    // 等待线程结束
    pthread_join(tid, NULL);

    printf("Main thread finished.\n");
    return 0;
}
```
给线程加上名称之后，就可以在崩溃的时候获取到线程名信息，可以快速定位崩溃点。也可以利用`ps -eLf | grep "my_custom_thread"/top`等命令查看线程信息。

##### d、stat_mem、/proc 等内存监控工具

当系统发生 `OOM` 时，可以使用`stat_mem`、`/proc/meminfo`等工具获取系统实时的内存变化状况，从而监测到具体为哪一个进程内存消耗过多。具体可以参考

[系统内存占用](../Debugging-Tools/内存工具/Linux内存工具.md#系统内存占用)



##### f、增加日志

当使用上面的工具都无法获得详细的崩溃行数或者获得的崩溃行数并未看到明显的错误时，还有一种最原始的方法。那就是通过在程序中增加打印，比如在调试的时候增加一些参数值的打印、增加一些循环的打印。最终确定崩溃的原因。


##### e、进程监控脚本 syslog.sh

监控一个进程的`syslog.sh`脚本

```bash
#!/bin/bash

# 监控的进程名
APP_NAME="your_app_name"  # 替换为你的应用程序名称
LOG_DIR="/dav2"           # 日志保存目录
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")  # 时间戳

# 检查日志目录是否存在，不存在则创建
mkdir -p "$LOG_DIR"

# 检查进程是否正在运行
if ! pgrep -x "$APP_NAME" > /dev/null; then
    echo "[$(date)] 进程 $APP_NAME 未运行" >> "$LOG_DIR/process_monitor.log"
    exit 1
fi

# 监控进程
while true; do
    if ! pgrep -x "$APP_NAME" > /dev/null; then
        echo "[$(date)] 检测到进程 $APP_NAME 崩溃" >> "$LOG_DIR/process_monitor.log"
        # 捕获dmesg输出并保存
        dmesg > "$LOG_DIR/dmesg_${APP_NAME}_${TIMESTAMP}.log"
        echo "[$(date)] dmesg日志已保存到 $LOG_DIR/dmesg_${APP_NAME}_${TIMESTAMP}.log" >> "$LOG_DIR/process_monitor.log"
        break
    fi
    sleep 5  # 每5秒检查一次
done
```


下面给出一个监控多进程的改进版`syslog.sh`文件

```bash
#!/bin/sh

# 定义日志文件路径
DMESG_LOG="/dav2/dmesgLog"
APP1_INFO="/home/app/app1info"
APP2_INFO="/home/app/app2info"

# 获取进程号函数
get_pid() {
    local process_name=\$1
    ps -w | awk -v name="$process_name" '\$0 ~ name && !/awk/ {print \$1}' | head -n 1
}

while true; do
    # 每次循环重新获取进程号，防止进程重启后pid变化
    app1_pid=$(get_pid "app1_name")
    app2_pid=$(get_pid "app2_name")

    # 监测app1进程
    if [ -n "$app1_pid" ] && kill -0 "$app1_pid" 2>/dev/null; then
        echo "$(date +'%Y-%m-%d %H:%M:%S'), pid=$app1_pid, app1_name process exists" > "$APP_INFO"
    else
        dmesg >> "$DMESG_LOG"
        echo "$(date +'%Y-%m-%d %H:%M:%S'), pid=$app1_pid, app1_name process not exists" >> "$DMESG_LOG"
        echo "$(date +'%Y-%m-%d %H:%M:%S'), pid=$app1_pid, app1_name process not exists"
        exit 5
    fi

    # 监测app2_pid进程
    if [ -n "$app2_pid" ] && kill -0 "$app2_pid" 2>/dev/null; then
        echo "$(date +'%Y-%m-%d %H:%M:%S'), pid=$app2_pid, app2_name process exists" > "$DSP_INFO"
    else
        dmesg >> "$DMESG_LOG"
        echo "$(date +'%Y-%m-%d %H:%M:%S'), pid=$app2_pid, app2_name process not exists" >> "$DMESG_LOG"
        echo "$(date +'%Y-%m-%d %H:%M:%S'), pid=$app2_pid, app2_name process not exists"
        exit 5
    fi

    usleep 100000
done
```

`syslog.sh`脚本可以手动在`SSH`中执行，也可以将这个脚本打包到设备的升级固件中，当设备在启动时，启动脚本`initrun.sh`中执行该脚本，就不需要每次都手动执行该脚本了。

##### f、守护进程 `daemonApp`






### 1.3、文件描述符泄漏和 `Socket`泄漏

将这两种泄漏类型放在一起，因为`Socket`也是属于文件描述符的范畴。不光是`Socket`，在 Linux 中，所有的`I/O`（文件、管道、`Socket`，设备等）都会被抽象成文件描述符。


操作系统会对进程可打开的句柄数量做限制，如 Linux 默认 1024/进程。泄漏会导致程序耗尽句柄，无法创建新连接或文件。泄漏的`Socket`可能占用网络端口，导致服务不可用。同时，每个句柄背后可能关联内核数据结构（如`Socket`缓冲区），这些结构会占用内存。大量泄漏可能导致系统内存压力上升，最终导致程序发生崩溃。

#### 1.3.1、描述符泄漏的原因

1. 文件描述符泄漏
```C
void read_file() {
    int fd = open("data.txt", O_RDONLY);  // 打开了文件
    // 使用fd后忘记close
}
int main() {
    for (int i = 0; i < 10000; i++) {
        read_file();  // 每次循环泄漏1个fd
    }
    // 最终: "Too many open files" 错误
}
```
2. `Socket`泄漏
```C
void connect_server() {
    int sock = socket(AF_INET, SOCK_STREAM, 0);  // 创建Socket
    struct sockaddr_in addr = {...};
    connect(sock, (struct sockaddr*)&addr, sizeof(addr));
    // 使用sock后没有close(sock)
}
int main() {
    while (1) {
        connect_server();  // 每次循环泄漏1个Socket
    }
    // 最终Service可能崩溃
}
```

#### 1.3.2、描述符泄漏的排查

##### a、lsof

使用 `lsof` 可以查看进程打开的所有文件描述符和网络连接

```bash
# 查看指定进程的打开文件
lsof -p <PID> 

# 筛选 TCP/UDP 连接
lsof -p <PID> | grep -E "TCP|UDP"

# 统计文件描述符数量
lsof -p <PID> | wc -l
```

##### b、/proc

`/proc`文件系统可以直接访问内核数据，实时查看文件描述符详情

```bash
# 查看进程所有打开的文件描述符
ls -l /proc/<PID>/fd

# 检查 socket inode 对应的连接
cat /proc/<PID>/net/tcp    # TCP 连接
cat /proc/<PID>/net/udp    # UDP 连接
```

##### c、netstat

使用 Linux 的 `netstat`工具，监控系统的`socket`套接字状态。


```bash
netstat -tunap
# 参数说明：
# -t：TCP 连接
# -u：UDP 连接
# -n：以数字形式显示地址和端口（避免 DNS 解析延迟）
# -a：显示所有连接（包括监听和非监听）
# -p：显示进程 PID 和名称

Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 192.168.1.10:80         192.168.1.20:54321      ESTABLISHED 1234/nginx
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      567/sshd
tcp        0      0 192.168.1.10:443        10.0.0.5:12345          CLOSE_WAIT  891/httpd
```

- `CLOSE_WAIT`：表示对方已关闭连接，但本地应用未调用 `close()`。
可能泄漏：大量 `CLOSE_WAIT` 表明应用未正确释放 `Socket`。
- `TIME_WAIT`：正常关闭的 `TCP` 连接残留，通常 `2MSL`（60秒）后自动消失。
需警惕：短时间内大量 `TIME_WAIT` 可能因频繁创建/销毁短连接（需优化连接复用）。
- `ESTABLISHED` 持续增长：可能未关闭已完成的连接。


也可以一直监控
```bash
netstat -tunap | awk '/^tcp/ {print $6}' | sort | uniq -c
10 ESTABLISHED
2 LISTEN
25 CLOSE_WAIT
5 TIME_WAIT
```
每隔一段时间运行此命令，若 `CLOSE_WAIT` 或 `ESTABLISHED` 数量持续增长，可能存在泄漏。

当排查出系统因为文件描述符泄露而间接导致的内存不足崩溃。这类崩溃的地方一般比较明确，就是在程序中描述符`open`和`close`不当导致的，此时只需要去对应地方的代码排查逻辑即可。


### 1.4、线程泄漏

> 线程泄露指程序中创建的线程未正确释放资源（如线程句柄、内核数据结构），导致系统资源逐渐耗尽，最终可能引发程序崩溃和性能下降。

#### 1.4.1、线程泄露的原因

1. 未回收可连接线程
    ```c
    #include <pthread.h>
    #include <stdio.h>

    void* thread_func(void* arg) {
        printf("Thread running\n");
        return NULL;
    }

    int main() {
        pthread_t tid;
        for (int i = 0; i < 100; i++) {
            pthread_create(&tid, NULL, thread_func, NULL); // 创建线程
            // 未调用 pthread_join 或 pthread_detach
        }
        sleep(60); // 模拟程序长期运行
        return 0;
    }
    ```
    循环创建100个线程后，所有线程资源未被回收，导致泄露。


#### 1.4.2、线程泄露的现象及后果

1. 通过 `ps -T -p <PID>` 或 `cat /proc/<PID>/status | grep Threads` 查看线程数会持续增长。
2. CPU占用率异常升高（大量线程竞争资源）。
3. 内存占用持续增长（线程栈未释放）


#### 1.4.3、线程泄露的排查

- 检查所有 `pthread_create` 是否匹配 `pthread_join` 或 `pthread_detach`。
- 确保异常分支（如 `return`、`goto`）调用了资源释放逻辑
- 使用 Linux 系统工具监控线程
    ```bash
    ps -T -p <PID>          # 列出进程的所有线程
    top -H -p <PID>         # 实时监控线程状态
    ```
### 1.5、锁泄漏

> 锁泄漏：指程序未正确释放持有的互斥锁（如 `pthread_mutex_t`）、读写锁或其他同步原语，导致锁资源无法被重用，进而引发线程阻塞、死锁或性能下降。

锁泄漏会比内存泄漏更加隐蔽，因为它不会直接导致程序崩溃，但会破坏多线程协作。

#### 1.5.1、锁泄漏的原因

1. 未配对的`lock/unlock`
    ```c
    #include <pthread.h>

    pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;

    void* thread_func(void* arg) {
        pthread_mutex_lock(&mutex);  // 加锁
        if (some_error_condition) {
            return NULL;  // 直接返回，未解锁！
        }
        // 正常操作...
        pthread_mutex_unlock(&mutex);  // 仅在正常路径解锁
        return NULL;
    }
    ```
    当 `some_error_condition` 成立时，锁未被释放，其他线程将永远无法获取该锁。

2. 重复加锁未释放

    ```c
    void critical_section() {
        pthread_mutex_lock(&mutex);  // 第一次加锁
        pthread_mutex_lock(&mutex);  // 重复加锁（非递归锁会阻塞自身！）
        // ...操作
        pthread_mutex_unlock(&mutex);  // 仅释放一次
    }
    ```
    若锁未设置为递归属性（`PTHREAD_MUTEX_RECURSIVE`），第二次加锁会导致线程永久阻塞自身；即使使用递归锁，若解锁次数不足，锁仍无法释放。

3. 动态锁未销毁，动态初始化的锁（如通过 `pthread_mutex_init` 创建）未调用 `pthread_mutex_destroy` 释放资源。
    ```c
    pthread_mutex_t mutex;
    pthread_mutex_init(&mutex, NULL);  // 动态初始化
    // ...使用锁
    // 忘记调用 pthread_mutex_destroy(&mutex);

    ```
    可能导致内存泄露（锁内部资源未释放）

#### 1.5.2、锁泄漏的现象及后果

- 线程阻塞：其他线程无法获取未释放的锁，永久等待。
- 死锁：多个锁泄露导致线程互相等待资源。
- 资源耗尽：大量未释放的锁占用系统资源（如内核对象）

#### 1.5.3、锁泄漏的排查

锁泄漏比较隐蔽，一般很难排查。下面提供几种解决思路。

1. 从源头开始预防
   1. 检查是否所有加锁都能匹配到解锁
   2. 检查所有的初始化锁都能匹配到销毁
   3. 使用`goto`同一清理锁资源。
2. 在日志中添加相关信息排查。
3. `GDB`在锁操作处打断点，观察锁的持有状态。

一般在产品级的嵌入式设备中，`GDB`的功能有限，只能分析`coredump`，不支持断点。因此，最常用的方法还是源头预防和加打印。

### 1.6、数据库泄漏

> 数据库泄漏指程序未能正确关闭数据库连接或者释放相关资源，导致数据库连接、游标或者内存长期占用，最终消耗系统资源。常见于`SQLite`，`MySQL`，`PostgreSQL`。


#### 1.6.1、数据库泄露的原因

以`SQLite3`为例，说明数据库泄漏

1. 连接未释放：数据库服务器端连接数不断增加，最终拒绝新连接。
    ```c
    #include <sqlite3.h>
    #include <stdio.h>

    int main() {
        sqlite3 *db;
        int rc;

        rc = sqlite3_open("test.db", &db);  // 打开数据库
        if (rc != SQLITE_OK) {
            fprintf(stderr, "无法打开数据库: %s\n", sqlite3_errmsg(db));
            return 1;
        }

        // 执行 SQL 但忘记关闭
        rc = sqlite3_exec(db, "CREATE TABLE users(id INT, name TEXT)", NULL, NULL, NULL);

        printf("操作完成，但没有关闭连接！\n");
        return 0;  // 程序退出，连接没关闭，SQLite 可能不会自动回收
    }
    ```
    没有调用`sqlite3_close(db)`，连接泄漏。多次连接之后耗尽`SQLite`的连接限制（默认可能只有256）

2. 未释放查询结果（游标泄漏）

    ```c
    #include <sqlite3.h>
    #include <stdio.h>

    int main() {
        sqlite3 *db;
        sqlite3_stmt *stmt;
        const char *sql = "SELECT * FROM users";

        sqlite3_open("test.db", &db);
        sqlite3_prepare_v2(db, sql, -1, &stmt, NULL);  // 准备查询

        while (sqlite3_step(stmt) == SQLITE_ROW) {
            printf("User ID: %d\n", sqlite3_column_int(stmt, 0));
        }

        // 忘记释放 stmt
        printf("查询结束，但没有释放 stmt！\n");
        sqlite3_close(db);
        return 0;
    }
    ```
    没有调用`sqlite3_finalize(stmt)`，游标未被释放，每次查询都会堆积游标，最终内存泄漏。

#### 1.6.2、数据库泄露的后果

数据库泄漏会带来的后果就是：

1. 数据库连接耗尽，数据库服务器拒绝新连接。
2. 新的查询请求无法执行，导致数据库服务中断。
3. 内存占用持续上升，`CPU/IO`因频繁创建连接而负载过高。

#### 1.6.3、数据库泄露的排查

数据库泄露的主要排查手段，还是在源头进行预防

1. 确保每个数据库句柄被`open()`之后，都有对应的`close()`
2. 确保每个数据库查询结构最后都会被释放


## 2、其他可能导致程序崩溃的原因

### 2.1、栈溢出

> 栈溢出指线程的调用栈空间（用于存储局部变量、函数调用信息）被耗尽，通常由 过深递归 或 超大局部变量 引发。

#### 2.1.1、栈溢出的原因

1. 无限递归
    ```c
    void infinite_recursion() {
        int local_var;  // 每次递归占用栈空间
        infinite_recursion(); // 无限递归，栈空间耗尽
    }

    int main() {
        infinite_recursion();
        return 0;
    }
    ```
    现象：程序崩溃，段错误

2. 大局部变量
    ```c
    void stack_heavy_function() {
        char buffer[1024 * 1024 * 10];  // 在栈上分配10MB空间（远超默认栈大小）
        // ...
    }

    int main() {
        stack_heavy_function();  // 触发栈溢出
        return 0;
    }
    ```
    现象：函数调用时立即崩溃，栈空间不足以容纳 `buffer`

3. 递归中分配大内存
    ```c
    void recursive_func(int depth) {
        char buffer[1024 * 1024];  // 每次递归分配1MB栈空间
        if (depth > 0) {
            recursive_func(depth - 1);  // 递归10次需要10MB栈空间（默认栈通常为8MB）
        }
    }

    int main() {
        recursive_func(10);
        return 0;
    }
    ```
    现象：递归到一定深度后崩溃，栈空间被逐步耗尽。

#### 2.1.2、栈溢出的排查

- 使用`GDB`查看崩溃时的调用栈
- 优化程序中的递归调用，确认有终止条件
- 避免在函数中使用过大的局部变量，改为使用堆内存。


### 2.2、浮点异常(`Undefined Signal 8`)

#### 2.2.1、浮点异常的原因

1. 除 0 操作
    ```c
    int a = 10 / 0;  // 触发 SIGFPE
    ```
2. 非法浮点运算，如对负数开平方
    ```c
    sqrt(-1.0);
    ```
3. 整数溢出:对超出类型范围的数值进行操作（如 `int` 类型超出 `[-2147483648, 2147483647]`
